---
title: "Sequential Sampling Modelling"
author: "Simone D'Ambrogio"
subtitle: ""
date: "`r Sys.Date()`"
output: 
  html_document:
    fig_caption: true
    toc: false
    toc_depth: 3
    toc_float: true
    theme: united
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, fig.align='left')
library(knitr)
## Global options
options(max.print="75")
opts_chunk$set(
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE
               )
opts_knit$set(width=75)
```

<style type="text/css">
  body {
    text-align: justify;
    font-size: 16px;
  }
  img[src$="centerme"] {
    display:block;
    margin: 0 auto;
  }
</style>

<br><br>

# {.tabset}

## Introduction

<br>

This file contains all the codes we used to fit the computational models.

<br><br>

Load libraries and data-set
```{r}

# - Home path: Salience - #
hp <- '../'

# Path From Home
PaFH <- function(pt) paste0(hp, pt)

# ---- Load Libraries ---- #
source( PaFH('utils/mytheme.R') )
library(brms); library(dplyr); library(purrr)
options( mc.cores = 4 )
library( rstan ); rstan_options(auto_write = TRUE)
library(ggplot2); theme_set(ggpubr::theme_pubr())
library(gridExtra)
pars <- function(x) return( c('log_lik', paste0('Intercept_', x)) )

# ---- Load Empirical Data ---- #
data_ddm <- read.csv(file = PaFH('Data/data_ddmGL.csv') ) %>% 
  rename(Vup=gain, Vlw=loss,
         Gup=gaze_gain, Glw=gaze_loss) %>% 
  mutate(rt = tft/1000)
```


<br><br>

Models

<br>
NM: Null Model, which corresponds to the classic DDM. <br>
AM: Attnetional Model, which corresponds to the classic aDDM. <br>
EEM: Exogenous vs Endogenous Model, which assumes that the attentional discounting effect (i.e., θ) is “different” in exogenous attention, relative to endogenous attention.<br>


## Fit

### Brightness & Font-Size

<br>

In this section we run the models combining the Brightness and Font-Size condition.

<br><br>


Create Stan Data
```{r stan-data}
#Stan Data
stan_data <- list(
  N        = nrow(data_ddm),
  Y        = data_ddm$rt,
  dec      = data_ddm$choice,   
  
  Vup      = data_ddm$Vup,
  Vlw      = data_ddm$Vlw,
  Gup      = data_ddm$Gup,
  Glw      = data_ddm$Glw,
  SalL     = data_ddm$SalL,
  
  prior_only = 0
)
```

<br><br>

#### Models {.tabset}

##### Null Model

<br>
Stan Code
```{stan stan-code-NM, output.var='NM'}
// generated with brms 2.16.3
functions {
   real wiener_diffusion_lpdf(real y, int dec, real alpha, 
                              real tau, real beta, real delta) { 
     if (dec == 1) {
       return wiener_lpdf(y | alpha, tau, beta, delta);
     } else {
       return wiener_lpdf(y | alpha, tau, 1 - beta, - delta);
     }
   }
}

data {
  int<lower=1> N;  // total number of observations
  vector[N] Y;  // response variable
  int<lower=0,upper=1> dec[N];  // decisions
  int prior_only;  // should the likelihood be ignored?
  
  vector[N] Vup;
  vector[N] Vlw;
  vector[N] Gup;
  vector[N] Glw;
  
}
transformed data {
  real min_Y = min(Y);
}
parameters {
  real<lower=0> Intercept_lambda;
  real<lower=0> Intercept_d;
  real<lower=0.05> Intercept_B;
  real<lower=0, upper=1> Intercept_z;  // temporary intercept for centered predictors
}

model {
  // likelihood including constants
  if (!prior_only) {
        
    vector[N] lambda = Intercept_lambda + rep_vector(0.0, N);
    vector[N] Lav;
    vector[N] d = Intercept_d + rep_vector(0.0, N);
    vector[N] th = 1 + rep_vector(0.0, N);
    vector[N] B = Intercept_B + rep_vector(0.0, N);
    vector[N] z = Intercept_z + rep_vector(0.0, N);
    vector[N] muU;
    vector[N] muL;
    vector[N] mu;
    
    for (n in 1:N) {
      
      Lav[n] = Vlw[n]*lambda[n]; 
      
      muU[n] = Gup[n] * d[n]*(Vup[n] - Lav[n]*th[n]);
      muL[n] = Glw[n] * d[n]*(Vup[n]*th[n] - Lav[n]);
      mu[n]  = muU[n] + muL[n];
    }
    
    for (n in 1:N) {
      target += wiener_diffusion_lpdf(Y[n] | dec[n], B[n], 0.0001, z[n], mu[n]);
    }
    
  }
  // priors including constants
  target += student_t_lpdf(Intercept_d | 3, 1, 2.5);
  target += normal_lpdf(Intercept_lambda | 1, 5);
  target += normal_lpdf(Intercept_B | 2, 3);
  target += beta_lpdf(Intercept_z | 3, 3);
  
}

generated quantities {
  vector[N] log_lik;
  
  vector[N] lambda = Intercept_lambda + rep_vector(0.0, N);
  vector[N] Lav;
  vector[N] d = Intercept_d + rep_vector(0.0, N);
  vector[N] th = 1 + rep_vector(0.0, N);
  vector[N] B = Intercept_B + rep_vector(0.0, N);
  vector[N] z = Intercept_z + rep_vector(0.0, N);
  vector[N] muU;
  vector[N] muL;
  vector[N] mu;
  
  for (n in 1:N) {
    
    Lav[n] = Vlw[n]*lambda[n]; 
    
    muU[n] = Gup[n] * d[n]*(Vup[n] - Lav[n]*th[n]);
    muL[n] = Glw[n] * d[n]*(Vup[n]*th[n] - Lav[n]);
    mu[n]  = muU[n] + muL[n];
  }
    
  for (n in 1:N) {
    log_lik[n] = wiener_diffusion_lpdf(Y[n] | dec[n], B[n], 0.0001, z[n], mu[n]);
  }
}
```

<br>
Fit the model
```{r fit-NM}
NM<-rstan::sampling(NM, iter = 2000, cores=4, refresh = 0,
                    pars=pars( c('d', 'B', 'z', 'lambda') ),
                    data=stan_data, save_warmup = FALSE)
```


##### Attentional Model

<br>
Stan Code
```{stan stan-code-AM, output.var='AM'}
// generated with brms 2.16.3
functions {
   real wiener_diffusion_lpdf(real y, int dec, real alpha, 
                              real tau, real beta, real delta) { 
     if (dec == 1) {
       return wiener_lpdf(y | alpha, tau, beta, delta);
     } else {
       return wiener_lpdf(y | alpha, tau, 1 - beta, - delta);
     }
   }
}

data {
  int<lower=1> N;  // total number of observations
  vector[N] Y;  // response variable
  int<lower=0,upper=1> dec[N];  // decisions
  int prior_only;  // should the likelihood be ignored?
  
  vector[N] Vup;
  vector[N] Vlw;
  vector[N] Gup;
  vector[N] Glw;
  
}
transformed data {
  real min_Y = min(Y);
}
parameters {
  real<lower=0> Intercept_lambda;
  real<lower=0> Intercept_d;
  real<lower=0, upper=1> Intercept_th;
  real<lower=0.05> Intercept_B;
  real<lower=0, upper=1> Intercept_z;  // temporary intercept for centered predictors
}

model {
  // likelihood including constants
  if (!prior_only) {
    
    vector[N] lambda = Intercept_lambda + rep_vector(0.0, N);
    vector[N] Lav;
    vector[N] d = Intercept_d + rep_vector(0.0, N);
    vector[N] th = Intercept_th + rep_vector(0.0, N);
    vector[N] B = Intercept_B + rep_vector(0.0, N);
    vector[N] z = Intercept_z + rep_vector(0.0, N);
    vector[N] muU;
    vector[N] muL;
    vector[N] mu;
    
    for (n in 1:N) {
      Lav[n] = Vlw[n]*lambda[n]; 
      
      muU[n] = Gup[n] * d[n]*(Vup[n] - Lav[n]*th[n]);
      muL[n] = Glw[n] * d[n]*(Vup[n]*th[n] - Lav[n]);
      mu[n]  = muU[n] + muL[n];
    }
    
    for (n in 1:N) {
      target += wiener_diffusion_lpdf(Y[n] | dec[n], B[n], 0.0001, z[n], mu[n]);
    }
    
  }
  // priors including constants
  target += student_t_lpdf(Intercept_d | 3, 1, 2.5);
  target += normal_lpdf(Intercept_lambda | 1, 5);
  target += beta_lpdf(Intercept_th | 2, 2);
  target += normal_lpdf(Intercept_B | 2, 3);
  target += beta_lpdf(Intercept_z | 3, 3);
  
}

generated quantities {
  vector[N] log_lik;
  
  vector[N] lambda = Intercept_lambda + rep_vector(0.0, N);
  vector[N] Lav;
  vector[N] d = Intercept_d + rep_vector(0.0, N);
  vector[N] th = Intercept_th + rep_vector(0.0, N);
  vector[N] B = Intercept_B + rep_vector(0.0, N);
  vector[N] z = Intercept_z + rep_vector(0.0, N);
  vector[N] muU;
  vector[N] muL;
  vector[N] mu;
  
  for (n in 1:N) {
    Lav[n] = Vlw[n]*lambda[n]; 
    
    muU[n] = Gup[n] * d[n]*(Vup[n] - Lav[n]*th[n]);
    muL[n] = Glw[n] * d[n]*(Vup[n]*th[n] - Lav[n]);
    mu[n]  = muU[n] + muL[n];
  }
    
  for (n in 1:N) {
    log_lik[n] = wiener_diffusion_lpdf(Y[n] | dec[n], B[n], 0.0001, z[n], mu[n]);
  }
}

```

<br>
Fit the model
```{r fit-AM}
AM<-rstan::sampling(AM, iter = 2000, cores=4, refresh = 0,
                    pars=pars( c('d', 'th','B', 'z', 'lambda') ),
                    data=stan_data, save_warmup = FALSE)
```

##### Ex-End Model

<br>
Stan Code
```{stan stan-code-EEM, output.var='EEM'}
// generated with brms 2.16.3
functions {
   real wiener_diffusion_lpdf(real y, int dec, real alpha, 
                              real tau, real beta, real delta) { 
     if (dec == 1) {
       return wiener_lpdf(y | alpha, tau, beta, delta);
     } else {
       return wiener_lpdf(y | alpha, tau, 1 - beta, - delta);
     }
   }
}
data {
  int<lower=1> N;  // total number of observations
  vector[N] Y;  // response variable
  int<lower=0,upper=1> dec[N];  // decisions
  int prior_only;  // should the likelihood be ignored?
  
  vector[N] SalL;
  vector[N] Vup;
  vector[N] Vlw;
  vector[N] Gup;
  vector[N] Glw;
  
}
transformed data {
  real min_Y = min(Y);
}
parameters {
  real<lower=0> Intercept_lambda;
  real<lower=0> Intercept_d;
  real<lower=0, upper=1> Intercept_Tex;
  real<lower=0, upper=1> Intercept_Tend;
  real<lower=0.05> Intercept_B;
  real<lower=0, upper=1> Intercept_z;  
}

model {
  // likelihood including constants
  if (!prior_only) {
    
    vector[N] lambda = Intercept_lambda + rep_vector(0.0, N);
    vector[N] Lav;
    vector[N] d = Intercept_d + rep_vector(0.0, N);
    vector[N] Tex = Intercept_Tex + rep_vector(0.0, N);
    vector[N] Tend = Intercept_Tend + rep_vector(0.0, N);
    vector[N] B = Intercept_B + rep_vector(0.0, N);
    vector[N] z = Intercept_z + rep_vector(0.0, N);
    vector[N] muU;
    vector[N] muL;
    vector[N] mu;
    
    for (n in 1:N) {
      Lav[n] = Vlw[n]*lambda[n]; 
      if(SalL[n]){
        muU[n] = Gup[n] * d[n]*(Vup[n] - Lav[n]*Tend[n]);
        muL[n] = Glw[n] * d[n]*(Vup[n]*Tex[n] - Lav[n]);
      } else {
        muU[n] = Gup[n] * d[n]*(Vup[n] - Lav[n]*Tex[n]);
        muL[n] = Glw[n] * d[n]*(Vup[n]*Tend[n] - Lav[n]);
      }
      
      mu[n]  = muU[n] + muL[n];
    }
    
    for (n in 1:N) {
      target += wiener_diffusion_lpdf(Y[n] | dec[n], B[n], 0.0001, z[n], mu[n]);
    }
    
  }
  // priors including constants
  target += student_t_lpdf(Intercept_d | 3, 1, 2.5);
  target += normal_lpdf(Intercept_lambda | 1, 5);
  target += beta_lpdf(Intercept_Tex | 2, 2);
  target += beta_lpdf(Intercept_Tend | 2, 2);
  target += normal_lpdf(Intercept_B | 2, 3);
  target += beta_lpdf(Intercept_z | 3, 3);
  
}

generated quantities {
  vector[N] log_lik;
  
  vector[N] lambda = Intercept_lambda + rep_vector(0.0, N);
  vector[N] Lav;
  vector[N] d = Intercept_d + rep_vector(0.0, N);
  vector[N] Tex = Intercept_Tex + rep_vector(0.0, N);
  vector[N] Tend = Intercept_Tend + rep_vector(0.0, N);
  vector[N] B = Intercept_B + rep_vector(0.0, N);
  vector[N] z = Intercept_z + rep_vector(0.0, N);
  vector[N] muU;
  vector[N] muL;
  vector[N] mu;
  
  for (n in 1:N) {
    Lav[n] = Vlw[n]*lambda[n]; 
    if(SalL[n]){
      muU[n] = Gup[n] * d[n]*(Vup[n] - Lav[n]*Tend[n]);
      muL[n] = Glw[n] * d[n]*(Vup[n]*Tex[n] - Lav[n]);
    } else {
      muU[n] = Gup[n] * d[n]*(Vup[n] - Lav[n]*Tex[n]);
      muL[n] = Glw[n] * d[n]*(Vup[n]*Tend[n] - Lav[n]);
    }
    
    mu[n]  = muU[n] + muL[n];
  }
    
  for (n in 1:N) {
    log_lik[n] = wiener_diffusion_lpdf(Y[n] | dec[n], B[n], 0.0001, z[n], mu[n]);
  }
}

```

<br>
Fit the model
```{r fit-EEM}
EEM<-rstan::sampling(EEM, iter = 2000, cores=4, refresh = 0,
                     pars=pars( c('d', 'Tex', 'Tend','B', 'z', 'lambda') ),
                     data=stan_data, save_warmup = FALSE)
```

#### {-}

<br>


<br><br><br>

### Brightness vs Font-Size

<br>

In this section we run the models accounting for the difference between the Brightness and Font-Size condition.

<br><br>


Create Stan Data
```{r stan-data1}
X <- model.matrix(~salience_type, data_ddm)

#Stan Data
stan_data <- list(
  N        = nrow(data_ddm),
  Y        = data_ddm$rt,
  dec      = data_ddm$choice,   
  X        = X,
  K        = ncol(X),
  Vup      = data_ddm$Vup,
  Vlw      = data_ddm$Vlw,
  Gup      = data_ddm$Gup,
  Glw      = data_ddm$Glw,
  SalL     = data_ddm$SalL,
  
  prior_only = 0
)
```

<br><br>

#### Models {.tabset}

##### Attentional Model

<br>
Stan Code
```{stan stan-code-AM1, output.var='AM1'}
// generated with brms 2.16.3
functions {
   real wiener_diffusion_lpdf(real y, int dec, real alpha, 
                              real tau, real beta, real delta) { 
     if (dec == 1) {
       return wiener_lpdf(y | alpha, tau, beta, delta);
     } else {
       return wiener_lpdf(y | alpha, tau, 1 - beta, - delta);
     }
   }
}

data {
  int<lower=1> N;  // total number of observations
  vector[N] Y;  // response variable
  int<lower=0,upper=1> dec[N];  // decisions
  int<lower=1> K;  // number of population-level effects (Brightness vs Font-Size)
  matrix[N, K] X;  // population-level design matrix
  int prior_only;  // should the likelihood be ignored?
  
  vector[N] Vup;
  vector[N] Vlw;
  vector[N] Gup;
  vector[N] Glw;
  
}
transformed data {
  real min_Y = min(Y);
}
parameters {
  real<lower=0> Intercept_lambda;
  real<lower=0> Intercept_d;
  vector<lower=0, upper=1>[K] Intercept_th;
  real<lower=0.05> Intercept_B;
  real<lower=0, upper=1> Intercept_z;  // temporary intercept for centered predictors
}

model {
  // likelihood including constants
  if (!prior_only) {
    
    vector[N] lambda = Intercept_lambda + rep_vector(0.0, N);
    vector[N] Lav;
    vector[N] d = Intercept_d + rep_vector(0.0, N);
    vector[N] th = X * Intercept_th;
    vector[N] B = Intercept_B + rep_vector(0.0, N);
    vector[N] z = Intercept_z + rep_vector(0.0, N);
    vector[N] muU;
    vector[N] muL;
    vector[N] mu;
    
    for (n in 1:N) {
      Lav[n] = Vlw[n]*lambda[n]; 
      
      muU[n] = Gup[n] * d[n]*(Vup[n] - Lav[n]*th[n]);
      muL[n] = Glw[n] * d[n]*(Vup[n]*th[n] - Lav[n]);
      mu[n]  = muU[n] + muL[n];
    }
    
    for (n in 1:N) {
      target += wiener_diffusion_lpdf(Y[n] | dec[n], B[n], 0.0001, z[n], mu[n]);
    }
    
  }
  // priors including constants
  target += student_t_lpdf(Intercept_d | 3, 1, 2.5);
  target += normal_lpdf(Intercept_lambda | 1, 5);
  target += beta_lpdf(Intercept_th | 2, 2);
  target += normal_lpdf(Intercept_B | 2, 3);
  target += beta_lpdf(Intercept_z | 3, 3);
  
}

generated quantities {
  vector[N] log_lik;
  
  vector[N] lambda = Intercept_lambda + rep_vector(0.0, N);
  vector[N] Lav;
  vector[N] d = Intercept_d + rep_vector(0.0, N);
  vector[N] th = X * Intercept_th;
  vector[N] B = Intercept_B + rep_vector(0.0, N);
  vector[N] z = Intercept_z + rep_vector(0.0, N);
  vector[N] muU;
  vector[N] muL;
  vector[N] mu;
  
  for (n in 1:N) {
    Lav[n] = Vlw[n]*lambda[n]; 
    
    muU[n] = Gup[n] * d[n]*(Vup[n] - Lav[n]*th[n]);
    muL[n] = Glw[n] * d[n]*(Vup[n]*th[n] - Lav[n]);
    mu[n]  = muU[n] + muL[n];
  }
    
  for (n in 1:N) {
    log_lik[n] = wiener_diffusion_lpdf(Y[n] | dec[n], B[n], 0.0001, z[n], mu[n]);
  }
}

```

<br>
Fit the model
```{r fit-AM1}
AM1<-rstan::sampling(AM1, iter = 2000, cores=4, refresh = 0,
                     pars=pars( c('d', 'th', 'B', 'z', 'lambda') ),
                     data=stan_data, save_warmup = FALSE)
```




##### Ex-End Model

<br>
Stan Code
```{stan stan-code-EEM1, output.var='EEM1'}
// generated with brms 2.16.3
functions {
   real wiener_diffusion_lpdf(real y, int dec, real alpha, 
                              real tau, real beta, real delta) { 
     if (dec == 1) {
       return wiener_lpdf(y | alpha, tau, beta, delta);
     } else {
       return wiener_lpdf(y | alpha, tau, 1 - beta, - delta);
     }
   }
}
data {
  int<lower=1> N;  // total number of observations
  vector[N] Y;  // response variable
  int<lower=0,upper=1> dec[N];  // decisions
  int<lower=1> K;  // number of population-level effects (Brightness vs Font-Size)
  matrix[N, K] X;  // population-level design matrix
  int prior_only;  // should the likelihood be ignored?
  
  vector[N] SalL;
  vector[N] Vup;
  vector[N] Vlw;
  vector[N] Gup;
  vector[N] Glw;
  
}
transformed data {
  real min_Y = min(Y);
}
parameters {
  real<lower=0> Intercept_lambda;
  real<lower=0> Intercept_d;
  vector<lower=0, upper=1>[K] Intercept_Tex;
  vector<lower=0, upper=1>[K] Intercept_Tend;
  real<lower=0.05> Intercept_B;
  real<lower=0, upper=1> Intercept_z;  
}

model {
  // likelihood including constants
  if (!prior_only) {
    
    vector[N] lambda = Intercept_lambda + rep_vector(0.0, N);
    vector[N] Lav;
    vector[N] d = Intercept_d + rep_vector(0.0, N);
    vector[N] Tex = X * Intercept_Tex;
    vector[N] Tend = X * Intercept_Tend;
    vector[N] B = Intercept_B + rep_vector(0.0, N);
    vector[N] z = Intercept_z + rep_vector(0.0, N);
    vector[N] muU;
    vector[N] muL;
    vector[N] mu;
    
    for (n in 1:N) {
      Lav[n] = Vlw[n]*lambda[n]; 
      if(SalL[n]){
        muU[n] = Gup[n] * d[n]*(Vup[n] - Lav[n]*Tend[n]);
        muL[n] = Glw[n] * d[n]*(Vup[n]*Tex[n] - Lav[n]);
      } else {
        muU[n] = Gup[n] * d[n]*(Vup[n] - Lav[n]*Tex[n]);
        muL[n] = Glw[n] * d[n]*(Vup[n]*Tend[n] - Lav[n]);
      }
      
      mu[n]  = muU[n] + muL[n];
    }
    
    for (n in 1:N) {
      target += wiener_diffusion_lpdf(Y[n] | dec[n], B[n], 0.0001, z[n], mu[n]);
    }
    
  }
  // priors including constants
  target += student_t_lpdf(Intercept_d | 3, 1, 2.5);
  target += normal_lpdf(Intercept_lambda | 1, 5);
  target += beta_lpdf(Intercept_Tex | 2, 2);
  target += beta_lpdf(Intercept_Tend | 2, 2);
  target += normal_lpdf(Intercept_B | 2, 3);
  target += beta_lpdf(Intercept_z | 3, 3);
  
}

generated quantities {
  vector[N] log_lik;
  
  vector[N] lambda = Intercept_lambda + rep_vector(0.0, N);
  vector[N] Lav;
  vector[N] d = Intercept_d + rep_vector(0.0, N);
  vector[N] Tex = X * Intercept_Tex;
  vector[N] Tend = X * Intercept_Tend;
  vector[N] B = Intercept_B + rep_vector(0.0, N);
  vector[N] z = Intercept_z + rep_vector(0.0, N);
  vector[N] muU;
  vector[N] muL;
  vector[N] mu;
  
  for (n in 1:N) {
    Lav[n] = Vlw[n]*lambda[n]; 
    if(SalL[n]){
      muU[n] = Gup[n] * d[n]*(Vup[n] - Lav[n]*Tend[n]);
      muL[n] = Glw[n] * d[n]*(Vup[n]*Tex[n] - Lav[n]);
    } else {
      muU[n] = Gup[n] * d[n]*(Vup[n] - Lav[n]*Tex[n]);
      muL[n] = Glw[n] * d[n]*(Vup[n]*Tend[n] - Lav[n]);
    }
    
    mu[n]  = muU[n] + muL[n];
  }
    
  for (n in 1:N) {
    log_lik[n] = wiener_diffusion_lpdf(Y[n] | dec[n], B[n], 0.0001, z[n], mu[n]);
  }
}

```

<br>
Fit the model
```{r fit-EEM1}
EEM1<-rstan::sampling(EEM1, iter = 2000, cores=4, refresh = 0,
                      pars=pars( c('d', 'Tex', 'Tend','B', 'z', 'lambda') ),
                      data=stan_data, save_warmup = FALSE)
```

#### {-}

<br>

## Compare

### Brightness & Font-Size

<br>
Compute Leave-One-Out
```{r loo-computation}
looNM  <- loo::loo(NM)
looAM  <- loo::loo(AM)
looEEM <- loo::loo(EEM)

looList <- list(NM=looNM, AM=looAM, EEM=looEEM)

loo_comp <- loo::loo_compare(looList) %>% as.data.frame()
```

<br>
Plot Model Comparison

```{r comparison-plot, message=F, warning=F, fig.width=3, fig.height=4}
library(RColorBrewer)
col <-  c('gray', brewer.pal(n = 9, 'BuPu')[c(6, 8)])

models <- c('NM', 'AM', 'EEM')
loo_comp$model <- row.names(loo_comp)
loo_comp %>% 
  #filter(model %in% models) %>% 
  mutate(delta_looic=min(looic)-looic - 50) %>% 
  ggplot( aes(reorder(model, delta_looic), delta_looic) ) +
  geom_bar( stat="identity", aes(fill=model)) +
  #coord_flip() +
  scale_fill_manual(breaks = models, values = col) +
  scale_y_continuous(breaks = seq(-900, 0, 300)-50, 
                     labels = seq(-900, 0, 300) ) +
  labs( y = "Δ LOOIC", x="", title='Figure 1') +
  mytheme() +
  theme( legend.position="none") 
  
```


### Brightness vs Font-Size

<br>
Compute Leave-One-Out
```{r loo-computation1}
looAM1  <- loo::loo(AM1)
looEEM1 <- loo::loo(EEM1)

looList <- list(NM=looNM, AM=looAM1, EEM=looEEM1)

loo_comp <- loo::loo_compare(looList) %>% as.data.frame()
```

<br>
Plot Model Comparison

```{r comparison-plot1, message=F, warning=F, fig.width=3, fig.height=4}
library(RColorBrewer)
col <-  c('gray', brewer.pal(n = 9, 'BuPu')[c(6, 8)])

models <- c('NM', 'AM', 'EEM')
loo_comp$model <- row.names(loo_comp)
loo_comp %>% 
  #filter(model %in% models) %>% 
  mutate(delta_looic=min(looic)-looic - 50) %>% 
  ggplot( aes(reorder(model, delta_looic), delta_looic) ) +
  geom_bar( stat="identity", aes(fill=model)) +
  #coord_flip() +
  scale_fill_manual(breaks = models, values = col) +
  scale_y_continuous(breaks = seq(-900, 0, 300)-50, 
                     labels = seq(-900, 0, 300) ) +
  labs( y = "Δ LOOIC", x="", title = "Figure 2") +
  mytheme() +
  theme( legend.position="none") 
  
```


### Brightness & Font-Size VS Brightness vs Font-Size

<br>
Plot Model Comparison

```{r comparison-plot2, message=F, warning=F, fig.width=4, fig.height=4}
library(RColorBrewer)
col <-  c('gray', 
          brewer.pal(n = 9, 'BuPu')[c(6, 8)],
          brewer.pal(n = 9, 'BuPu')[c(6, 8)]
          )

models <- c('NM', 'AM', 'EEM', 'AM1', 'EEM1')
looList <- list(NM=looNM, 
                AM=looAM, EEM=looEEM,
                AM1=looAM1, EEM1=looEEM1)

loo_comp <- loo::loo_compare(looList) %>% as.data.frame()

loo_comp$model <- row.names(loo_comp)
loo_comp %>% 
  #filter(model %in% models) %>% 
  mutate(delta_looic=min(looic)-looic - 50) %>% 
  ggplot( aes(reorder(model, delta_looic), delta_looic) ) +
  geom_bar( stat="identity", aes(fill=model)) +
  #coord_flip() +
  scale_fill_manual(breaks = models, values = col) +
  scale_y_continuous(breaks = seq(-900, 0, 300)-50, 
                     labels = seq(-900, 0, 300) ) +
  labs( y = "Δ LOOIC", x="", title = "Figure 3") +
  mytheme() +
  theme( legend.position="none") 
  
```

<br>

AM1 and EEM1 are the models that account for the difference between the brightness and the font-size group.


## Posteriors

### Brightness & Font-Size

<br>

```{r}
# Parameters List
pl = list( 
  NM = c("d", "B", "z", "lambda"),
  AM = c("d", "th","B", "z", "lambda"),
  EEM = c("d", "Tex", "Tend", "B", "z", "lambda"),
  AM1 = c("d", "th[1]", "th[2]", "B", "z", "lambda"),
  EEM1 = c("d", "Tex[1]", "Tex[2]", "Tend[1]", "Tend[2]", "B", "z", "lambda")
)
make_par <- function(par) paste0('Intercept_', par)
```

<br>

#### Models {.tabset}

##### Null Model

```{r posteriors-NM}
#Change name parameters
names(NM)[names(NM) %in% make_par(pl$NM)] <- pl$NM

stan_dens(NM, pars =  pl$NM) + labs(title = "Figure 1")
```

##### Attentional Model

```{r posteriors-AM}
#Change name parameters
names(AM)[names(AM) %in% make_par(pl$AM)] <- pl$AM

stan_dens(AM, pars =  pl$AM) + labs(title = "Figure 2")
```

##### Ex-End Model

```{r posteriors-EEM}
#Change name parameters
names(EEM)[names(EEM) %in% make_par(pl$EEM)] <- pl$EEM

stan_dens(EEM, pars =  pl$EEM) + labs(title = "Figure 3")
```

#### {-}

<br><br>

### Brightness vs Font-Size

<br>

#### Models {.tabset}

##### Attentional Model

```{r posteriors-AM1}
#Change name parameters
newnames <- c('d', 'th_B', 'th_FS-th_B', 'B', 'z', 'lambda')
names(AM1)[names(AM1) %in% make_par(pl$AM1)] <- newnames
  
stan_dens(AM1, pars =  newnames) + labs(title = "Figure 4")
```

<br>

Look only at Tex and Tend 

```{r posteriors-A1_2, fig.width=6, fig.height=4}
#Change name parameters
am1 <-  extract(AM1)

df_pl <- rbind(
  data.frame(post=am1$Intercept_th[,1], sal_type = 'Brightness'),
  data.frame(post=am1$Intercept_th[,1]+am1$Intercept_th[,2], sal_type = 'Font-Size')
)

df_pl %>% 
  ggplot(aes(post, color=sal_type, fill=sal_type)) +
  geom_density(alpha=0.6) +
  mytheme() + labs(color='', fill='', title = "Figure 5")  +
  scale_color_manual(values =  c("#DB2763", "#17377A")) +
  scale_fill_manual( values = c("#DB2763", "#17377A"))
```

##### Ex-End Model

```{r posteriors-EEM1, fig.width=9, fig.height=7}
#Change name parameters
newnames <- c('d', 'Tex_B', 'Tex_FS-Tex_B', 'Tend_B', 'Tend_FS-Tend_B', 'B', 'z', 'lambda')
names(EEM1)[names(EEM1) %in% make_par(pl$EEM1)] <- newnames

stan_dens(EEM1, pars =  newnames) + labs(title = "Figure 6")
```

<br>

Look only at Tex and Tend 

```{r posteriors-EEM1_2, fig.width=8.5, fig.height=4}
#Change name parameters
eem1 <-  extract(EEM1)

df_pl_end <- rbind(
  data.frame(post=eem1$Intercept_Tend[,1], sal_type = 'Brightness'),
  data.frame(post=eem1$Intercept_Tend[,1]+eem1$Intercept_Tend[,2], sal_type = 'Font-Size')
) %>% mutate(th_type='Tend')

df_pl_ex <- rbind(
  data.frame(post=eem1$Intercept_Tex[,1], sal_type = 'Brightness'),
  data.frame(post=eem1$Intercept_Tex[,1]+eem1$Intercept_Tex[,2], sal_type = 'Font-Size')
) %>% mutate(th_type='Tex')

rbind(df_pl_ex, df_pl_end) %>% 
  ggplot(aes(post, color=sal_type, fill=sal_type)) +
  geom_density(alpha=0.6) +
  facet_wrap(~th_type) +
  mytheme() + labs(color='', fill='')  +
  scale_color_manual(values =  c("#DB2763", "#17377A")) +
  scale_fill_manual( values = c("#DB2763", "#17377A")) + labs(title = "Figure 7")
```

#### {-}

# {-}


